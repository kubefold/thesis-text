%! suppress = MissingLabel
\chapter{Results}
%
%\section{Implementation of the KubeFold Project}
%
%Within the project, the following goals were successfully accomplished:
%\begin{itemize}
%    \item Designed and implemented the \textit{KubeFold} operator for the Kubernetes platform.
%    \item Created two custom resources (CRDs):
%    \begin{itemize}
%        \item \textit{ProteinDatabase} – managing protein databases,
%        \item \textit{ProteinConformationPrediction} – running protein conformation prediction computations.
%    \end{itemize}
%    \item Developed a component for automatic downloading and decompression of protein databases.
%    \item Prepared infrastructure as code (IaC) allowing for quick operator deployment in AWS cloud (EKS + FSx for Lustre).
%    \item Created documentation and installation instructions for the Kubernetes operator.
%    \item Conducted several protein conformation computations to test the solution.
%\end{itemize}


\section{Comparison of manual and automated AlphaFold installation}

To fully understand the advantages of the KubeFold operator, it is essential to compare the time and effort required for manual installation versus automated deployment using Kubernetes.

\subsection{Time requirements for manual AlphaFold setup}

Manual installation of AlphaFold on an EC2 Ubuntu instance requires significant time investment across several stages:

\begin{enumerate}
    \item \textbf{Infrastructure setup} (30-60 minutes):
    \begin{itemize}
        \item Launching an appropriate EC2 instance (typically p3.2xlarge with NVIDIA GPU)
        \item Configuring and attaching storage volumes (minimum 3TB)
        \item Setting up network access and security groups
    \end{itemize}
    
    \item \textbf{Environment configuration} (30-60 minutes):
    \begin{itemize}
        \item Installing NVIDIA drivers and CUDA toolkit
        \item Setting up Docker and NVIDIA Container Toolkit
        \item Configuring appropriate permissions and environment variables
    \end{itemize}
    
    \item \textbf{Protein database acquisition} (70 minutes):
    \begin{itemize}
        \item Downloading multiple large databases (BFD, MGnify, PDB70, PDB mmCIF, UniRef30, UniProt, UniRef90)
        \item Total download size approximately 415GB (2.62TB after extraction)
    \end{itemize}
    
    \item \textbf{Software configuration} (60-180 minutes):
    \begin{itemize}
        \item Building the AlphaFold Docker image
        \item Troubleshooting CUDA library compatibility issues
        \item Testing GPU detection and functionality
        \item Configuring input and output paths
    \end{itemize}
    
    \item \textbf{Protein structure prediction} (25 minutes):
    \begin{itemize}
        \item Running the actual prediction process
        \item Managing computational resources
    \end{itemize}
\end{enumerate}

The total time required for manual setup ranges from 3-5 hours, excluding any troubleshooting that might be necessary. Most critically, this process requires continuous user interaction and technical expertise in Linux system administration, GPU configuration, and containerization.

\subsection{KubeFold automated deployment process}

In contrast, the KubeFold operator streamlines this entire process into a few simple steps:

\begin{enumerate}
    \item \textbf{Cluster creation} (50 minutes):
    \begin{itemize}
        \item Setting up a Kubernetes cluster in AWS (EKS)
        \item This process is automated through infrastructure as code
        \item Requires minimal user interaction
    \end{itemize}
    
    \item \textbf{Operator installation} (5 minutes):
    \begin{itemize}
        \item Deploying the KubeFold operator to the cluster
        \item Setting up necessary permissions and service accounts
    \end{itemize}
    
    \item \textbf{Protein database acquisition} (70 minutes):
    \begin{itemize}
        \item Creating a \texttt{ProteinDatabase} custom resource
        \item Automated downloading and processing of all required databases
        \item Parallel downloading for optimal performance
    \end{itemize}
    
    \item \textbf{Protein structure prediction} (25 minutes):
    \begin{itemize}
        \item Creating a \texttt{ProteinConformationPrediction} custom resource
        \item Automated execution of the prediction process
        \item Results storage and notification
    \end{itemize}
\end{enumerate}

The total time for the KubeFold approach is approximately 150 minutes (2.5 hours), with the significant advantage that the entire process requires virtually no user intervention beyond the initial resource creation. The operator handles all complex tasks automatically, including resource provisioning, job scheduling, and result management.

\subsection{Comparative analysis}

Table~\ref{tab:installation_comparison} presents a direct comparison between manual and KubeFold-based AlphaFold installation approaches:

\begin{table}[H]
    \centering
    \small
    \begin{tabularx}{\textwidth}{|X|X|X|}
        \hline
        \textbf{Aspect} & \textbf{Manual Installation} & \textbf{KubeFold Approach} \\
        \hline
        \textbf{Total setup time} & 3-5 hours & 2.5 hours \\
        \hline
        \textbf{Required user interaction} & Continuous involvement throughout the process & Minimal (only initial resource creation) \\
        \hline
        \textbf{Technical expertise required} & High (Linux, Docker, CUDA, GPU configuration) & Basic Kubernetes knowledge only \\
        \hline
        \textbf{Error susceptibility} & High (many manual steps) & Low (automated processes) \\
        \hline
        \textbf{Scalability} & Limited to single machine & Highly scalable across cluster nodes \\
        \hline
        \textbf{Resource optimization} & Manual management & Automatic allocation and cleanup \\
        \hline
        \textbf{Reproducibility} & Challenging (environment variations) & Consistent (declarative approach) \\
        \hline
    \end{tabularx}
    \caption{Comparison of manual and KubeFold-based AlphaFold installation}
    \label{tab:installation_comparison}
\end{table}

\subsection{Key advantages of the KubeFold approach}

The KubeFold operator provides several significant advantages over manual installation:

\begin{itemize}
    \item \textbf{Zero interaction requirement:} After initial resource creation, the entire process runs without user intervention, freeing researchers to focus on scientific work rather than infrastructure management.
    
    \item \textbf{Error reduction:} By automating complex setup procedures, KubeFold eliminates common errors in environment configuration and dependency management.
    
    \item \textbf{Resource optimization:} The operator intelligently allocates computational resources, separating CPU-intensive alignment tasks from GPU-intensive prediction tasks to optimize cost and performance.
    
    \item \textbf{Scalability:} Multiple predictions can be run in parallel across cluster nodes, significantly increasing throughput for large-scale studies.
    
    \item \textbf{Reproducibility:} The declarative, Kubernetes-based approach ensures consistent environments and results across different executions.
    
    \item \textbf{Integration with cloud services:} Native integration with AWS services like S3, FSx for Lustre, and SNS provides robust storage, high-performance file systems, and automated notifications.
\end{itemize}

These advantages make KubeFold particularly valuable for research teams that need to run multiple protein structure predictions without dedicating significant time and expertise to infrastructure management. The platform effectively bridges the gap between the computational complexity of AlphaFold and the practical needs of biological researchers.

\section{Comparison of alternative solutions}

The AlphaFold algorithm has significantly influenced the development of bioinformatics, offering breakthrough capabilities in predicting protein spatial structures.
However, its practical use comes with significant computational and technological requirements.
In response to these challenges, alternative solutions have emerged, such as \textbf{OpenFold}\cite{openfold} and \textbf{ColabFold}\cite{colabfold}, which aimed to simplify the prediction process and increase its accessibility.
However, \textbf{KubeFold} – the system developed as part of this work – stands out among these tools, providing a much higher level of automation, scalability, and cloud infrastructure integration.

\subsection*{OpenFold}

OpenFold is an open--source reimplementation of AlphaFold2, written in Python using the PyTorch library.
This project is primarily targeted at research teams and institutions that have extensive computational infrastructure and need full control over the prediction process.
OpenFold enables, among other things, training models from scratch, testing architecture modifications, and running tasks in distributed environments (HPC, cloud).

Due to the advanced nature of the tool, its implementation requires high expertise in machine learning, container management, and cloud environment configuration.
Significant hardware resources (e.g., A100--class GPU cards) are also necessary, which may be an entry barrier for smaller teams.

\subsection*{ColabFold}

ColabFold is a simplified version of AlphaFold2, available through interactive notebooks running in the Google Colaboratory environment.
Thanks to integration with the MMseqs2 tool, the sequence alignment process is much faster, and users don't need to configure the environment or install software themselves.

ColabFold is primarily aimed at individuals and teams that don't have advanced computational infrastructure.
Despite the low entry threshold, this solution has numerous limitations: it doesn't allow model training, is dependent on Colab platform limitations (session time, GPU availability), and its application is mainly limited to simple, single predictive tasks.

\subsection*{Advantages of the KubeFold Approach}

The \textbf{KubeFold} platform developed as part of this work represents a modern and flexible alternative to the above solutions.
By utilizing Kubernetes technology and integration with Amazon Web Services (such as FSx for Lustre and S3), KubeFold automates the entire process of running protein conformation predictions – from downloading databases, through running calculations, to archiving results and sending notifications.

Unlike OpenFold, KubeFold doesn't require manual task configuration or infrastructure management – all components are automatically launched based on YAML resources, which significantly simplifies the process.
Compared to ColabFold, the KubeFold platform offers much greater scalability and control over computational resources – allowing for simultaneous running of multiple tasks and allocating appropriate resources (CPU, GPU, memory, disk space).
%TODO(matisiekpl): I don't know if this is enough
This scalability is achieved due to being built on top of Kubernetes platform.

This approach combines the advantages of flexibility and computational power with ease of use, making KubeFold a more comprehensive solution adapted to the real needs of research environments and production applications.

A detailed comparison of the computational infrastructure for OpenFold, ColabFold, and KubeFold is presented in Table~\ref{tab:comparison}.

\begin{table}[H]
    \centering
    \small
    \begin{tabularx}{\textwidth}{|X|X|X|X|}
        \hline
        \textbf{Features}                           & \textbf{OpenFold}                         & \textbf{ColabFold}                & \textbf{KubeFold (proposed)}                          \\
        \hline
        \textbf{Platform}                           & HPC clusters, cloud (AWS, GCP)            & Google Colab, local environments & Kubernetes + AWS EKS \\
        \hline
        \textbf{Hardware requirements}              & High (GPU A100, RAM 64GB+)                & Low (free GPU in Colab) & Medium--high, dynamically scalable \\
        \hline
        \textbf{Automation}                         & Manual task and environment configuration & None, manual handling by user & Full automation through Kubernetes operator \\
        \hline
        \textbf{Scalability}                        & High, but difficult to configure          & Limited to single predictions & Very high, supports parallel processing \\
        \hline
        \textbf{Ease of deployment}                 & High entry threshold (DevOps, ML)         & Very easy, suitable for beginners & Medium threshold, requires basic Kubernetes knowledge \\
        \hline
        \textbf{Integration with external services} & Requires own implementation               & None                              & Yes – AWS S3, FSx, SNS, IAM                           \\
        \hline
    \end{tabularx}
    \caption{Comparison of computational infrastructure: OpenFold, ColabFold, and KubeFold}
    \label{tab:comparison}
\end{table}

\section{Challenges}

A significant architectural challenge involved designing the KubeFold platform to operate efficiently across multiple cluster nodes while optimizing costs and computation speed.
By dividing the download process into multiple pods, it was possible to increase the potential throughput of protein database downloads.
Additionally, separating the \textit{Aligning} and \textit{Predicting} phases allowed for optimal utilization of CPU and GPU computational resources.
This approach enables substantial cost savings in the public cloud environment.

Implementing the \textit{downloader} component presented another technical hurdle.
This component needed to simultaneously download and decompress data to minimize storage requirements.
If these processes were to occur sequentially, the temporarily used disk space would be much larger than the actual size of the database, creating potential resource constraints.

Setting up appropriate infrastructure for testing the KubeFold platform proved particularly demanding.
The project relies on multiple AWS cloud services, requiring thoughtful integration of each component.
Configuring appropriate subnet settings, VPC connections, and CSI drivers required significant time investment and troubleshooting.

From a developer experience (DX) perspective, KubeFold represents a careful compromise between automation and customization. Each decision to automate a process inherently limits the flexibility available to end users. For example, the automated database download process optimizes for parallel throughput but restricts users from customizing download sources or prioritizing specific databases. Similarly, the separation of alignment and prediction phases improves resource utilization but reduces the ability to fine-tune the computational pipeline for specialized cases.

The most conceptually challenging aspect was determining the appropriate level of abstraction for the Kubernetes operator. Making the system too generic would require extensive configuration from users, defeating the purpose of automation. Conversely, making it too specific would limit its applicability across different research scenarios. The final design represents a balance that favors automation for common workflows while still providing configuration options for essential parameters.

Another significant trade-off was between cloud vendor integration and platform independence. While deep integration with AWS services like FSx for Lustre provided performance benefits, it created dependencies that limit deployment options. Considerable effort went into designing interfaces that could potentially support alternative storage solutions in future iterations without requiring major architectural changes.

\subsection{Fault tolerance and failure handling}

During testing of the KubeFold platform, several scenarios involving job failures were encountered, providing valuable insights into the system's resilience and error handling capabilities. One particularly instructive case occurred during a protein conformation prediction where the prediction job failed due to insufficient GPU memory allocation.

The failure manifested during the \textit{Predicting} phase when the AlphaFold container attempted to load the neural network model onto a GPU with insufficient VRAM. The job was initially scheduled on a G4dn instance with an NVIDIA T4 GPU (16GB VRAM), which proved insufficient for the model requirements. The Kubernetes job status showed \texttt{Failed > 0}, triggering the operator's automated failure handling mechanism. Upon detection of the failure, the system exhibited the following behavior:

\begin{enumerate}
    \item \textbf{Automatic failure detection:} The controller immediately detected the job failure through regular status polling of the Kubernetes Job resource.
    
    \item \textbf{Retry mechanism activation:} Since this was the first failure (retry count = 0), the system automatically incremented the retry counter and deleted the failed job to prepare for a retry attempt.
    
    \item \textbf{Resource cleanup and recreation:} The failed job pod was terminated and removed, while the persistent data (including MSA results from the previous aligning phase) remained intact on the shared storage volume.
    
    \item \textbf{Automatic retry execution:} A new prediction job was created with identical configuration, leveraging the preserved alignment data to avoid repeating the time-intensive MSA generation process.
\end{enumerate}

However, the second attempt also failed with the same GPU memory error, followed by a third failure. After reaching the maximum retry limit of 3 attempts, the system transitioned the \texttt{ProteinConformationPrediction} resource to the \texttt{Failed} status with the error message: \textit{"Prediction job failed after max retries"}.

This failure scenario highlighted several important aspects of the KubeFold design:

\begin{itemize}
    \item \textbf{State preservation:} The alignment results were preserved between retry attempts, preventing unnecessary recomputation of the MSA which would have added significant overhead (approximately 15 minutes per retry).
    
    \item \textbf{Clear error reporting:} The system provided clear status information through both the Kubernetes resource status and operator logs, making it straightforward to diagnose the underlying cause.
    
    \item \textbf{Resource cleanup:} Failed jobs were automatically cleaned up to prevent cluster resource accumulation, while preserving data necessary for debugging.
    
    \item \textbf{Graceful degradation:} The system failed predictably without affecting other concurrent predictions or the overall cluster stability.
\end{itemize}

The resolution involved modifying the node selector configuration to target G5 instances with NVIDIA A10G GPUs (24GB VRAM instead of the original 16GB T4), after which subsequent predictions completed successfully. This experience demonstrated that while the automated retry mechanism effectively handles transient failures (such as temporary resource unavailability), it appropriately stops retrying when encountering persistent configuration issues that require manual intervention.

The fault tolerance mechanisms extend beyond prediction jobs to cover all phases of the workflow. Similar retry logic applies to the alignment phase (MSA generation) and artifact upload phase, ensuring that temporary network issues or storage problems do not result in complete workflow failures. This comprehensive approach to error handling significantly improves the platform's reliability in production environments where various transient failures are common.

\section{Performance analysis and cost evaluation}

To provide a comprehensive assessment of the KubeFold platform's efficiency, this section presents detailed timing analysis for each computational phase and cost evaluation compared to alternative approaches.

\subsection{Execution time breakdown}

Table~\ref{tab:phase_timing} presents the execution times for individual phases of protein conformation prediction using the KubeFold operator. The measurements were taken during the test execution described in the previous sections.

\begin{table}[H]
    \centering
    \small
    \begin{tabularx}{\textwidth}{|X|X|X|X|}
        \hline
        \textbf{Phase} & \textbf{Instance Type} & \textbf{Duration (minutes)} & \textbf{Resource Requirements} \\
        \hline
        \textbf{Database Download} & m5.xlarge & 70 & 4 vCPU, 16 GB RAM, High network throughput \\
        \hline
        \textbf{Aligning (MSA)} & m5.xlarge & 15 & 4 vCPU, 16 GB RAM, FSx Lustre access \\
        \hline
        \textbf{Predicting} & g5.xlarge & 25 & 4 vCPU, 16 GB RAM, 1x NVIDIA A10G GPU \\
        \hline
        \textbf{Uploading Artifacts} & m5.xlarge & 3 & 4 vCPU, 16 GB RAM, S3 write access \\
        \hline
        \textbf{Total Prediction Time} & - & \textbf{43} & - \\
        \hline
        \textbf{Total Including Setup} & - & \textbf{113} & - \\
        \hline
    \end{tabularx}
    \caption{Execution time breakdown for KubeFold protein conformation prediction phases}
    \label{tab:phase_timing}
\end{table}

The database download phase represents a one-time cost that can be amortized across multiple predictions. For subsequent predictions using the same database, the total execution time reduces to approximately 43 minutes.

\subsection{Cost analysis}

Based on AWS pricing in the eu-central-1 region (as of January 2025)~\cite{aws-ec2-pricing,aws-fsx-pricing}, Table~\ref{tab:cost_analysis} provides a detailed cost breakdown for running protein conformation predictions using KubeFold compared to alternative approaches.

\begin{table}[H]
    \centering
    \small
    \begin{tabularx}{\textwidth}{|X|X|X|X|}
        \hline
        \textbf{Component} & \textbf{KubeFold Cost} & \textbf{Manual EC2 Cost} & \textbf{SageMaker Cost*} \\
        \hline
        \textbf{Compute - CPU phases} & \$0.19/hour × 1.3h = \$0.25 & \$0.19/hour × 1.3h = \$0.25 & \$0.23/hour × 1.3h = \$0.30 \\
        \hline
        \textbf{Compute - GPU prediction} & \$1.00/hour × 0.42h = \$0.42 & \$1.00/hour × 0.42h = \$0.42 & \$1.20/hour × 0.42h = \$0.50 \\
        \hline
        \textbf{Storage (FSx Lustre)} & \$0.13/GB-month × 3TB × 2h = \$1.08 & \$0.13/GB-month × 3TB × 4h = \$2.16 & \$0.20/GB-month × 3TB × 2h = \$1.67 \\
        \hline
        \textbf{Data Transfer (ingress)} & Free (downloading TO AWS) & Free (downloading TO AWS) & Free \\
        \hline
        \textbf{S3 Storage} & \$0.02/GB × 5GB = \$0.10 & \$0.02/GB × 5GB = \$0.10 & \$0.02/GB × 5GB = \$0.10 \\
        \hline
        \textbf{Total per prediction} & \textbf{\$1.85} & \textbf{\$2.93} & \textbf{\$2.57} \\
        \hline
        \textbf{Total (database reuse)} & \textbf{\$1.85} & \textbf{\$2.93} & \textbf{\$2.57} \\
        \hline
    \end{tabularx}
    \caption{Cost comparison for protein conformation prediction approaches}
    \label{tab:cost_analysis}
\end{table}

*SageMaker costs are estimated based on the AWS blog reference~\cite{aws-sagemaker-protein-folding}, assuming optimized managed instances and included database storage.

\textbf{Note:} All pricing data is based on official AWS pricing pages~\cite{aws-ec2-pricing,aws-fsx-pricing} accessed on January 9, 2025, for the EU Central (Frankfurt) region. FSx for Lustre costs are calculated by converting monthly pricing (\$0.13/GB-month) to hourly rates (\$0.00018/GB-hour) and multiplying by storage size and duration. Ingress data transfer (downloading databases TO AWS) is free when using public subnets with internet gateways~\cite{aws-ec2-pricing}. This analysis assumes no NAT Gateway usage, making database downloads completely free from a data transfer perspective.

\subsection{Performance comparison with AWS SageMaker approach}

The AWS SageMaker protein folding workflow described in the referenced blog post~\cite{aws-sagemaker-protein-folding} provides a useful benchmark for comparing KubeFold's performance. Table~\ref{tab:sagemaker_comparison} presents this comparison.

\begin{table}[H]
    \centering
    \small
    \begin{tabularx}{\textwidth}{|X|X|X|}
        \hline
        \textbf{Aspect} & \textbf{AWS SageMaker Pipeline} & \textbf{KubeFold Operator} \\
        \hline
        \textbf{MSA Generation Time} & ~15 minutes (ml.m5.xlarge) & 15 minutes (m5.xlarge) \\
        \hline
        \textbf{Structure Prediction Time} & ~25 minutes (ml.g5.xlarge) & 25 minutes (g5.xlarge) \\
        \hline
        \textbf{Setup Complexity} & High (SageMaker Studio, custom containers) & Medium (Kubernetes knowledge) \\
        \hline
        \textbf{Database Management} & Manual S3 setup and FSx configuration & Automated through operator \\
        \hline
        \textbf{Scalability} & High (SageMaker Pipelines) & High (Kubernetes native) \\
        \hline
        \textbf{Cost per prediction} & \$2.57 & \$1.85 \\
        \hline
        \textbf{Vendor Lock-in} & High (AWS SageMaker specific) & Medium (Kubernetes portable, AWS services) \\
        \hline
    \end{tabularx}
    \caption{Performance comparison between AWS SageMaker and KubeFold approaches}
    \label{tab:sagemaker_comparison}
\end{table}

\subsection{Cost optimization analysis}

The KubeFold platform achieves cost optimization through several key mechanisms:

\begin{itemize}
    \item \textbf{Resource separation:} By using separate instance types for CPU-intensive alignment and GPU-intensive prediction phases, the platform avoids paying for unused GPU time during database operations.
    
    \item \textbf{Ephemeral compute:} Pods are automatically terminated after task completion, ensuring no idle compute costs.
    
    \item \textbf{Database reuse:} The protein database can be reused for multiple predictions without any additional data transfer costs, maintaining the consistent \$1.85 per-prediction cost.
    
    \item \textbf{Parallel processing:} Multiple predictions can run simultaneously across cluster nodes, improving throughput without proportional cost increases.
\end{itemize}

The database download incurs no data transfer charges since ingress traffic to AWS is free when using public subnets with internet gateways~\cite{aws-ec2-pricing}. This means that both initial and subsequent predictions have the same cost structure (\$1.85 per prediction), making KubeFold cost-effective for research teams running protein structure predictions.

When compared to manually managed EC2 instances using the same instance types, KubeFold shows a 37\% cost reduction for all predictions. This efficiency gain comes from automated resource lifecycle management and reduced idle time. Compared to AWS SageMaker, KubeFold achieves a 28\% cost reduction while providing greater flexibility and reduced vendor lock-in. This demonstrates the platform's economic viability for both occasional and high-throughput usage scenarios.

\section{Conclusions}

The KubeFold project successfully addresses the challenges of protein structure prediction by leveraging modern cloud technologies and Kubernetes orchestration.
The implementation demonstrates that complex scientific computations can be effectively managed through a declarative, operator--based approach, significantly reducing operational overhead.
The platform's architecture, combining AWS services like FSx for Lustre and S3 with Kubernetes orchestration, provides a scalable and cost--effective solution for running AlphaFold predictions.
The separation of database management and computation phases, along with automated resource provisioning and cleanup, makes the system particularly suitable for research environments requiring high--throughput protein structure predictions.
This work demonstrates how cloud--native technologies can transform complex scientific workflows into manageable, automated processes, potentially accelerating discoveries in structural biology and drug development.